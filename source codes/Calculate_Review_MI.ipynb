{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import string\n",
    "from stemming.porter2 import stem\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(),'data','Movie_category_indexing.json')\n",
    "with open(data_path) as f:\n",
    "    movie_category = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(),'data','result.json')\n",
    "with open(data_path) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从txt文件中读取并重新构建字典：words_dic\n",
    "def read_words_indexing():\n",
    "    data_path = os.path.join(os.getcwd(),'review_indexing.txt')\n",
    "    file_read = open(data_path,'r',encoding='utf-8')\n",
    "    \n",
    "    words_dic={}# key is word, value is {doc_ID,[word_pos1,word_pos2,...]}\n",
    "    word=''\n",
    "    for line in file_read.readlines():\n",
    "        if line[0] != '\\t':\n",
    "            word = line.split(':')[0]\n",
    "            word_count = line.split(':')[1]\n",
    "            \n",
    "        elif line[0] == '\\t':\n",
    "            line = line.strip()\n",
    "            doc_ID = line.split(\" : \")[0]\n",
    "            word_pos = line.split(\" : \")[1].split(\",\")\n",
    "            for i in range(0,len(word_pos)):\n",
    "                word_pos[i] = int(word_pos[i])\n",
    "            if word not in words_dic.keys():#如果这个单词不在字典里\n",
    "                words_dic.update({word:{doc_ID : word_pos}})#一二级字典都添加\n",
    "            elif word in words_dic.keys():#如果这个单词在字典里\n",
    "                words_dic[word].update({doc_ID : word_pos})#只更新二级字典\n",
    "                \n",
    "    return words_dic\n",
    "\n",
    "words_dic = read_words_indexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MI(N_11, N_10, N_01, N_00):  # calculate Mutual Information\n",
    "    N = N_11 + N_10 + N_01 + N_00\n",
    "    if N_11 != 0:\n",
    "        term_1 = (N_11 / N) * math.log2((N * N_11) / ((N_10 + N_11) * (N_01 + N_11)))\n",
    "    else:\n",
    "        term_1 = 0\n",
    "\n",
    "    if N_01 != 0:\n",
    "        term_2 = (N_01 / N) * math.log2((N * N_01) / ((N_00 + N_01) * (N_01 + N_11)))\n",
    "    else:\n",
    "        term_2 = 0\n",
    "\n",
    "    if N_10 != 0:\n",
    "        term_3 = (N_10 / N) * math.log2((N * N_10) / ((N_10 + N_11) * (N_00 + N_10)))\n",
    "    else:\n",
    "        term_3 = 0\n",
    "\n",
    "    if N_00 != 0:\n",
    "        term_4 = (N_00 / N) * math.log2((N * N_00) / ((N_00 + N_01) * (N_00 + N_10)))\n",
    "    else:\n",
    "        term_3 = 0\n",
    "\n",
    "    MI = term_1 + term_2 + term_3 + term_4\n",
    "    return MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_English_stop_words():\n",
    "    data_path = os.path.join(os.getcwd(),'data','englishST.txt')\n",
    "    ref_file = open(data_path, \"r\")\n",
    "    English_stop_words = []#store English stop words\n",
    "    for ref_line in ref_file.readlines():\n",
    "        English_stop_words.append(ref_line.strip())\n",
    "    return(English_stop_words)\n",
    "\n",
    "English_stop_words = load_English_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocss_review(review_content):\n",
    "    English_stop_words = load_English_stop_words()\n",
    "    \n",
    "    #Delete punctuations\n",
    "    punctuations = list(string.punctuation)\n",
    "    punctuations.append('—')\n",
    "    punctuations.append('\\n')\n",
    "    for p in punctuations:\n",
    "        review_content = review_content.replace(p, '').strip()\n",
    "        \n",
    "    #split review into words\n",
    "    words = review_content.split(' ')\n",
    "            \n",
    "    # To low case\n",
    "    for i in range(0,len(words)):  \n",
    "        words[i] = words[i].lower()\n",
    "            \n",
    "    #Remove English stop words\n",
    "    words = [elem for elem in words if elem not in English_stop_words]\n",
    "#     print(words)\n",
    "            \n",
    "    #Stemming\n",
    "    for i in range(0,len(words)):\n",
    "        words[i]=stem(words[i].lower())\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430283"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_review_code():#return all the exising review code\n",
    "    all_review_code=[]\n",
    "    for key_movie,value_movie in data.items():\n",
    "#         print('Movie Name: ',key_movie)\n",
    "        for key_year,value_year in value_movie.items():\n",
    "            for review in value_year[2]:\n",
    "                review_code=key_movie+'_'+key_year+'_'+review['review_id']\n",
    "                all_review_code.append(review_code)\n",
    "    all_review_code = list(set(all_review_code)) #去重（为什么会有重复的项？）\n",
    "    return all_review_code\n",
    "            \n",
    "#get_all_review_code()[0]\n",
    "len(get_all_review_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeSecond(elem):#用来排序\n",
    "    return elem[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__光是计算一个review里所有word的MI就要大约45s（当然这和review的长度有关），如果要计算所有review，粗略估计要花费5000小时<br>__\n",
    "虽然还可以优化，但是我估计如果要提前计算所有的review_MI还是不太现实，建议加一个网页专门用来计算某一篇review的MI（提前告知用户要等待1分钟才能出结果）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def review_MI(review_code):#review_code的格式是 'movieName_year_reviewID'\n",
    "    words_MI=[]\n",
    "    line = review_code\n",
    "    movie,year,review_id = line.split('_')\n",
    "#     print(movie,year,review_id)\n",
    "    \n",
    "    #Get the category of the movie of this review\n",
    "    this_review_movie_category = data[movie][year][1].split(',')\n",
    "#     print('this_review_movie_category = ',this_review_movie_category)\n",
    "    \n",
    "    # Get review_content from the dictionary\n",
    "    review_content = None\n",
    "    for review in data[movie][year][2]:\n",
    "        if review['review_id'] == review_id:\n",
    "            review_content = review[\"review_detail\"]\n",
    "            \n",
    "    # Preprocessing the review_content\n",
    "    words = preprocss_review(review_content)\n",
    "    words = list(set(words))#去重\n",
    "    \n",
    "    # Calculate MI for each word\n",
    "    for word in words:# For each word\n",
    "#         print('word =',word)      \n",
    "#         print(words_dic[word])\n",
    "        \n",
    "        N_11,N_10,N_01,N_00 = 0,0,0,0 #paramters of MI\n",
    "        \n",
    "        categories=[]# categories of movie reviews that the word appears in\n",
    "        for review_code in words_dic[word].keys():# For each review that this word appears\n",
    "            #movie,year = review_code.split('_')[0:2]#存在这种review_code \"#cats_the_mewvie_2020_rw5574325\",所有这种办法会导致报错\n",
    "            p=review_code.find(\"_rw\")#找到\"_rw\"的位置\n",
    "            movie = review_code[0:p-5]\n",
    "            year = review_code[p-4:p]\n",
    "#             print('movie=',movie,', year=',year)\n",
    "            line = data[movie][year][1]# category line of this movie in the movie dataset\n",
    "            for category in line.split(','):\n",
    "                if category not in categories:\n",
    "                    categories.append(category)\n",
    "            current_review_category = line.split(',')\n",
    "#             print(current_review_category)\n",
    "#             print(set(this_review_movie_category).issubset(current_review_category))\n",
    "            if set(this_review_movie_category).issubset(current_review_category):\n",
    "                N_11+=1 #word出现在多少篇属于此类的doc中\n",
    "            else:\n",
    "                N_01+=1 #word出现在多少篇不属于此类的doc中\n",
    "        \n",
    "#         print('The word appears in categories = ',categories)\n",
    "        \n",
    "        \n",
    "        #找出属于此类的电影：\n",
    "        same_type_movies_set = {}\n",
    "        for category in this_review_movie_category:\n",
    "            if same_type_movies_set == {}:\n",
    "                same_type_movies_set = set(movie_category[category])\n",
    "            elif same_type_movies_set != {}:\n",
    "                same_type_movies_set = same_type_movies_set&set(movie_category[category])\n",
    "#         print('len(same_type_movies_set) = ',len(same_type_movies_set))\n",
    "        \n",
    "        #找出这些电影的影评的review_code\n",
    "        all_review_codes_belongsToThisCategory=[]#属于此类的doc\n",
    "        same_type_movies_set = list(same_type_movies_set)\n",
    "        for movie_code in same_type_movies_set:\n",
    "            movie,year = movie_code.split(\"_\")\n",
    "            \n",
    "            for review in data[movie][year][2]:\n",
    "                review_id = review['review_id']\n",
    "                review_code = movie_code+'_'+review_id\n",
    "                all_review_codes_belongsToThisCategory.append(review_code)\n",
    "#                 print('review_id=',review_id)\n",
    "#                 print('review_code=',review_code)\n",
    "#         print(all_review_codes_belongsToThisCategory)\n",
    "\n",
    "        #找出所有未出现此word的影评（在属于此类的doc中）\n",
    "        word_appears_review_codes = list(words_dic[word].keys())#出现过此词的doc\n",
    "        word_NOT_appears_review_codes = list( set(all_review_codes_belongsToThisCategory) - set(word_appears_review_codes) )\n",
    "        N_10 = len(word_NOT_appears_review_codes)\n",
    "#         print('len(all_review_codes_belongsToThisCategory)=',len(all_review_codes_belongsToThisCategory))\n",
    "#         print('len(word_appears_review_codes)=',len(word_appears_review_codes))\n",
    "\n",
    "        #在不属于此类的doc中有多少篇未出现此词\n",
    "        all_review_codes = get_all_review_code()\n",
    "#         print('len(all_review_codes)=',len(all_review_codes))\n",
    "        all_review_codes_NOT_belongsToThisCategory = list(set(all_review_codes)-set(all_review_codes_belongsToThisCategory))\n",
    "#         print('len(all_review_codes_NOT_belongsToThisCategory)=',len(all_review_codes_NOT_belongsToThisCategory))\n",
    "        N_00 = len(all_review_codes_NOT_belongsToThisCategory) - len(set(all_review_codes_NOT_belongsToThisCategory)&set(word_appears_review_codes))\n",
    "        \n",
    "        \n",
    "#         print('N_11=',N_11,'\\nN_10=',N_10,'\\nN_01=',N_01,'\\nN_00=',N_00)\n",
    "        word_MI = calculate_MI(N_11, N_10, N_01, N_00)\n",
    "#         print('word_MI = ',word_MI)\n",
    "        words_MI.append([word,word_MI])\n",
    "#         print(words_MI)\n",
    "#         break\n",
    "    words_MI.sort(key=takeSecond, reverse=True)#排序\n",
    "    return words_MI\n",
    "    \n",
    "\n",
    "review_MI_example = review_MI(\"niagara falls_1941_rw2493287\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['film', 0.0014616869184980712],\n",
       " ['marri', 0.0013581552746259962],\n",
       " ['scene', 0.0008401077258319957],\n",
       " ['coupl', 0.0006610227002213365],\n",
       " ['final', 0.0005490431380410737],\n",
       " ['interest', 0.0005222729963865569],\n",
       " ['good', 0.00046981182323250267],\n",
       " ['make', 0.0004673248449894689],\n",
       " ['fall', 0.00046170091623077745],\n",
       " ['mind', 0.0004439255002758925],\n",
       " ['dont', 0.0004272063762318064],\n",
       " ['point', 0.000415115991995731],\n",
       " ['back', 0.0004064396351419171],\n",
       " ['start', 0.00039713246602490933],\n",
       " ['pitt', 0.00038112191382855276],\n",
       " ['escap', 0.0003543381715698879],\n",
       " ['fact', 0.0003141689870645695],\n",
       " ['car', 0.00031332985101126145],\n",
       " ['year', 0.00029151627848146046],\n",
       " ['attract', 0.00023442590496584544],\n",
       " ['attempt', 0.00022586793345104306],\n",
       " ['complet', 0.0001970656809437254],\n",
       " ['wait', 0.00017860540763841855],\n",
       " ['man', 0.0001764442016803109],\n",
       " ['absolut', 0.0001713101529091164],\n",
       " ['attent', 0.00016163104441688616],\n",
       " ['produc', 0.00014640509444661877],\n",
       " ['funniest', 0.00013914432609119756],\n",
       " ['larg', 0.00013651313962034816],\n",
       " ['ridicul', 0.0001204587584792185],\n",
       " ['hood', 0.00011109754153084975],\n",
       " ['side', 0.00010954539227123433],\n",
       " ['pay', 0.00010925030848094703],\n",
       " ['strang', 9.870485783362295e-05],\n",
       " ['road', 9.319211389621963e-05],\n",
       " ['lead', 8.60148578279373e-05],\n",
       " ['room', 8.52579759112118e-05],\n",
       " ['sweetheart', 8.092295091148173e-05],\n",
       " ['hotel', 7.587287070706948e-05],\n",
       " ['tom', 7.184729664574008e-05],\n",
       " ['situat', 6.715904881215084e-05],\n",
       " ['climb', 6.40724296446613e-05],\n",
       " ['hide', 6.391709742009448e-05],\n",
       " ['carri', 6.217993804860666e-05],\n",
       " ['suit', 5.591092606981908e-05],\n",
       " ['tool', 5.529041206580137e-05],\n",
       " ['help', 5.5044136135495316e-05],\n",
       " ['brown', 5.374833627165855e-05],\n",
       " ['newlyw', 5.184132296128373e-05],\n",
       " ['theyr', 4.985652077995278e-05],\n",
       " ['spoil', 4.451804969838496e-05],\n",
       " ['pretend', 4.063484960640883e-05],\n",
       " ['eye', 3.956218889425282e-05],\n",
       " ['except', 3.9348012489210705e-05],\n",
       " ['sam', 3.791203285765277e-05],\n",
       " ['rest', 3.669888666847773e-05],\n",
       " ['encount', 3.4705232375422616e-05],\n",
       " ['assum', 3.3739465700858326e-05],\n",
       " ['trade', 3.27537696011026e-05],\n",
       " ['troubl', 3.1922499017267866e-05],\n",
       " ['setup', 3.18005475520832e-05],\n",
       " ['bed', 3.175097473908376e-05],\n",
       " ['young', 3.140464552933741e-05],\n",
       " ['play', 2.875563029487759e-05],\n",
       " ['consist', 2.8742476862411243e-05],\n",
       " ['window', 2.825070600038593e-05],\n",
       " ['youthi', 2.6560056361224332e-05],\n",
       " ['bridal', 2.600084615307426e-05],\n",
       " ['persist', 2.0491009991675623e-05],\n",
       " ['jacket', 1.894041984553924e-05],\n",
       " ['streamlin', 1.6370538479311993e-05],\n",
       " ['silli', 1.2824395757767931e-05],\n",
       " ['reconcil', 1.0564619629987425e-05],\n",
       " ['stranger', 1.043450819224469e-05],\n",
       " ['normalthi', 9.249614240655174e-06],\n",
       " ['pleasantnoth', 9.038576861644331e-06],\n",
       " ['pittsthough', 9.038576861644331e-06],\n",
       " ['43minut', 9.038576861644331e-06],\n",
       " ['samsummervill', 9.038576861644331e-06],\n",
       " ['woodworth', 9.038576861644331e-06],\n",
       " ['eventually\\x85wel', 9.038576861644331e-06],\n",
       " ['goofi', 7.908007583408056e-06],\n",
       " ['margi', 6.882963893502212e-06],\n",
       " ['zasu', 5.3974988767720775e-06],\n",
       " ['emmi', 5.162251411284138e-06],\n",
       " ['perfect', 4.998066562212158e-06],\n",
       " ['buttheyr', 4.6247607978403195e-06],\n",
       " ['cuddl', 4.0940788402473885e-06],\n",
       " ['slim', 3.6137754542934776e-06],\n",
       " ['insult', 3.2694202988262045e-06],\n",
       " ['niagara', 3.2264238149880456e-06],\n",
       " ['hilariouswhat', 3.1046512957971848e-06],\n",
       " ['pretens', 2.5530590787469274e-06],\n",
       " ['natur', 2.497327511467314e-06],\n",
       " ['summervill', 2.291855405693403e-06],\n",
       " ['pajama', 1.8543455111536262e-06],\n",
       " ['elseth', 1.4303850137056145e-06],\n",
       " ['lie', 1.3808930231608621e-06],\n",
       " ['reenter', 9.285325098613196e-07],\n",
       " ['hal', 6.820127401955795e-07],\n",
       " ['vigor', 6.644758556703541e-07],\n",
       " ['ledg', 5.707332193039076e-07],\n",
       " ['separ', 5.090582899174973e-07],\n",
       " ['fiddl', 3.4496602888703417e-07],\n",
       " ['marjori', 3.013198612226764e-07],\n",
       " ['quicki', 2.1826791734483058e-07],\n",
       " ['roach', 2.1546069503123998e-07],\n",
       " ['twenti', 2.5821788553605994e-08],\n",
       " ['root', 1.1421800117916195e-08],\n",
       " ['revolv', 1.9620415811429436e-09],\n",
       " ['watch', 1.3954117969780404e-11]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_MI_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
